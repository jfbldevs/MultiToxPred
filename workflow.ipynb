{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "datos = pd.read_csv('file.csv')\n",
    "\n",
    "X = datos.drop(['PRED'], axis=1)\n",
    "y = datos['PRED']\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases antes de dividir los datos en conjuntos de entrenamiento y prueba\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Create a new DataFrame with the balanced data\n",
    "balanced_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_data['PRED'] = y_resampled\n",
    "\n",
    "# Save the balanced data to a CSV file\n",
    "balanced_data.to_csv('file_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular descriptor computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from propy import PyPro\n",
    "from propy.GetProteinFromUniprot import GetProteinSequence\n",
    "from propy.GetProteinFromUniprot import GetProteinSequenceFromTxt as gpst\n",
    "from Bio import SeqIO\n",
    "\n",
    "desc_var = 'file'\n",
    "paac_list = []\n",
    "container = open(f'{desc_var}.fasta','r')\n",
    "for record in SeqIO.parse(container, \"fasta\"):#file\n",
    "    DesObject = PyPro.GetProDes(record.seq)\n",
    "\n",
    "    calc = DesObject.GetPAAC(lamda=5,weight=0.05)\n",
    "    #calc = DesObject.GetDPComp()\n",
    "\n",
    "    \n",
    "    paac_list.append(list(calc.values()))\n",
    "\n",
    "# Create a pandas DataFrame from the results\n",
    "df = pd.DataFrame(paac_list)\n",
    "\n",
    "# Generate custom column names with numbers and letters\n",
    "column_names = []\n",
    "for i in range(len(df.columns)):\n",
    "    column_name = f\"Desc_{i+1}\"\n",
    "    column_names.append(column_name)\n",
    "\n",
    "# Assign custom column names to the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "# Write the DataFrame to a CSV file with custom column names\n",
    "df.to_csv(f'{desc_var}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "datos = pd.read_csv('file_results.csv')\n",
    "\n",
    "X = datos.drop(['PRED'], axis=1)\n",
    "y = datos['PRED']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "#model1 = XGBClassifier(random_state=42)\n",
    "\n",
    "#model2 = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "#model = StackingClassifier(estimators=[('rf', model1), ('ann', model2)]).fit(X_train, y_train)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "# Perform cross-validation\n",
    "scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=10, return_train_score=True)\n",
    "\n",
    "# Extract the average scores\n",
    "accuracy = scores['test_accuracy'].mean()\n",
    "F1_score = scores['test_f1_macro'].mean()\n",
    "precision = scores['test_precision_macro'].mean()\n",
    "sensitivity_recall = scores['test_recall_macro'].mean()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1_score:\", F1_score)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Sensitivity/Recall:\", sensitivity_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular las métricas en los datos de prueba\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "f1_score_test = f1_score(y_test, y_pred, average='macro')\n",
    "precision_test = precision_score(y_test, y_pred, average='macro')\n",
    "recall_test = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir las métricas en los datos de prueba\n",
    "print(\"Accuracy (Testing):\", accuracy_test)\n",
    "print(\"F1_score (Testing):\", f1_score_test)\n",
    "print(\"Precision (Testing):\", precision_test)\n",
    "print(\"Sensitivity/Recall (Testing):\", recall_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation predictions\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "\n",
    "y_train_probs = cross_val_predict(model, X_train, y_train, cv=10, method='predict_proba')\n",
    "# Compute the AUC for the training set\n",
    "auc_train = roc_auc_score(y_train, y_train_probs, multi_class='ovr')\n",
    "print(\"Training AUC:\", auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# Compute the AUC\n",
    "auc = roc_auc_score(y_test, y_probs, multi_class='ovr')  # assuming multi-class classification\n",
    "\n",
    "print(\"Testing AUC:\", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('Tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c48e9b199503b64bfe0baf3512a6cae5718a891dc30dcc7ca8061b6fc888e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
